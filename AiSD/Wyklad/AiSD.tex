\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[polish]{babel}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage[margin=1in]{geometry}




\title{Algorytmy i Struktury Danych}
\author{Wojciech Typer}
\date{}

\begin{document}
\maketitle

\begin{algorithm}[H]
\caption{Insertion Sort}\label{alg:insertion_sort}
\begin{algorithmic}[1]
\Procedure{InsertionSort}{A, n}
    \For{$i = 1$ to $n-1$}
        \State $key = A[i]$
        \State $j = i - 1$
        \While{$j \geq 0$ and $A[j] > key$}
            \State $A[j+1] = A[j]$
            \State $j = j - 1$
        \EndWhile
        \State $A[j+1] = key$
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm} 
\vspace{1\baselineskip}
\textbf{Złożoność czasowa:} $O(n^2)$ \par
\textbf{Best case:} w najlepszym przypadku złożoność czasowa będzie wynosić $O(n)$ \par
\textbf{Złożoność pamięciowa:} $O(1)$
\vspace{2\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/wojteq18/Pobrane/zdjecia/insert-sort.png}
    \label{fig:example_image}
\end{figure}
\vspace{3\baselineskip}

\begin{algorithm}[H]
    \caption{Merge Sort}\label{alg:merge_sort}
    \begin{algorithmic}[1]
    \Procedure{MergeSort}{A, 1, n}
        \If{|A[1..n]| == 1} 
            \State \Return{A[1..n]}
        \Else
            \State $B = \text{MergeSort}(A, 1, \lfloor n/2 \rfloor)$
            \State $C = \text{MergeSort}(A, \lfloor n/2 \rfloor, n)$
            \State \Return{Merge(B, C)}
        \EndIf
    \EndProcedure
    \end{algorithmic}    
\end{algorithm}
\begin{algorithm}[H]
    \caption{Merge}\label{alg:merge}
    \begin{algorithmic}[1]
    \Procedure{Merge}{X[1..k], Y[1..n]}
        \If{$X = \emptyset$}
            \State \Return{$Y$}
        \ElsIf{$Y = \emptyset$}
            \State \Return{$X$}
        \ElsIf{$X[1] \leq Y[1]$}
            \State \Return{$[X[1]] \times \text{Merge}(X[2..k], Y[1..n])$}   
        \Else
            \State \Return{$[Y[1]] \times \text{Merge}(X[1..k], Y[2..n])$}
        \EndIf
    \EndProcedure
    \end{algorithmic}       
\end{algorithm}
\vspace{1\baselineskip}
\textbf{Złożoność czesowa Merge Sort:} $O(n \log n)$ \par
\textbf{Złożoność pamięciowa Merge Sort:} $O(n)$
\vspace{2\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/Merge_sort_algorithm_diagram.svg.png}
    \label{fig:example_image}
\end{figure}
\vspace{2\baselineskip} \par
Istnieje również iteracyjna wersja algorytmu Merge, sort, która została \par
przedstawiona poniżej w postaci pseudokodu.
\begin{algorithm}[H]
    \caption{IterativeMergeSort}\label{alg:iterative_merge}
    \begin{algorithmic}[1]
        \Procedure{IterativeMergeSort}{A[1..n]}
            \For{$size = 1$ \textbf{to} $n-1$ \textbf{by} $size \times 2$}
                \For{$left = 0$ \textbf{to} $n-1$ \textbf{by} $2 \times size$}
                    \State $mid \gets \min(left + size - 1, n - 1)$
                    \State $right \gets \min(left + 2 \times size - 1, n - 1)$
                    \State \Call{Merge}{A, left, mid, right}
                \EndFor
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\vspace{1\baselineskip}
\textbf{Złożoność czasowa Iterative Merge Sort:} $O(n \log n)$ - dzieje się tak, \par
ponieważ size jest podwajany o 2 w każdej iteracji, więc potrzebujemy \par
około $ \log_2 n$ iteracji, a w każdej z nich wykonujemy $O(n)$ operacji. \par
\vspace{1\baselineskip}
\textbf{Złożoność pamięciowa Iterative Merge Sort:} $O(n)$ \par
\vspace{1\baselineskip}
\textbf{Notacja asymptotyczna}
O:f(n) = O(g(n)) $\rightarrow (\exists c > 0) (\exists n_0 \in N) : \forall n \geq n_0 \rightarrow 0 \leq f(n) \leq c \cdot g(n)$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/images.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
$f(n) = O(g(n)) \rightarrow lim_{n \to \infty} \frac{|f(n)|}{|g(n)|} < \infty$ \par
\vspace{1\baselineskip}
\textbf{Notacja asymptotyczna - własności} \par
\vspace{1\baselineskip}
a) $f(n) = n^3 + O(n^2) \rightarrow (\exists h(n) = O(n^2))(f(n) = n^3 + h(n))$ \par
\vspace{1\baselineskip}
b) $n^2 + O(n) = O(n^2) \rightarrow (\forall f(n) = O(n))(\exists h(n) = O(n^2))(n^2 + f(n)-h(n))$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\Omega$} \par
\vspace{1\baselineskip}
$f(n) = \Omega (g(n)) \rightarrow(\exists c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(c * g(n) \leq |f(n))$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/1a7d9c3f882e7a237b30f5eb6defa1aa45c6ab22.png}
    \label{fig:example_image}
\end{figure}
\vspace{7\baselineskip}
\textbf{Notacja $\Omega$ - własności} \par
\vspace{1\baselineskip}
a) $n^3 = \Omega (2n^2)$ \par
\vspace{1\baselineskip}
b) $n = \Omega (log(n))$ \par
\vspace{1\baselineskip}
c) $2n^2 = \Omega (n^2)$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\Theta$} \par
\vspace{1\baselineskip}
$f(n) = \Theta (g(n)) \rightarrow (\exists c_1, c_2 > 0)(\exists n_0 \in N)(\forall n  \geq n_0)(c_1 g(n) \leq $ \par $|f(n) \leq c_2 g(n))$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/theeta.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
$\Theta (f(n)) = O(f(n)) \cap \Omega (f(n))$ \par
\vspace{1\baselineskip}
\textbf{Notacja o- małe} \par
\vspace{1\baselineskip}
$f(n) = o(g(n)) \rightarrow (\forall c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(|f(n)| < c * |g(n)|)$ \par
\vspace{1\baselineskip}
\textbf{Notacja o- małe - przykłady} \par
\vspace{1\baselineskip}
a) $117n log(n) = o(n^2)$ \par
\vspace{1\baselineskip}
b) $ n^2 = o(n^3)$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\omega$} \par
\vspace{1\baselineskip}
$f(n) = \omega (g(n)) \rightarrow (\forall c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(|f(n)| > c * |g(n)|)$ \par
\vspace{1\baselineskip}
$lim_{n \to \infty} \frac{|f(n)|}{|g(n)|} = \infty$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/allnotations.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
\textbf{Rekurencje} \par
\vspace{1\baselineskip}
Metoda podstawiania (metoda dowodzenia indukcyjnego) \par
    \hspace{20pt}1. Zgadnij odpowiedź (bez stałych) \par
    \hspace{20pt}2. Sprawdź przez indukcję, czy dobrze zgadliśmy \par
    \hspace{20pt}3. Znajdź stałe \par
\vspace{1\baselineskip}
Przykład 1: \par
\vspace{1\baselineskip}
$T(n) = 4T(\frac{n}{2}) + n$   \par
Pierwszy strzał: $T(n) = O(n^3)$ \par
Cel: pokazać, że $(\exists c > 0) T(n) \leq c * n^3$ \par
Krok początkowy: $T(1) = \Theta (1) = c * 1^3 = c$ \par
Krok indukcyjny: zał. że, $(\forall_(k < n)) (T(k) \leq c * k^3) = $ \par
Dowód: $T(n) = 4T(\frac{n}{2}) + n \leq 4c * (\frac{n}{2})^3 + n = \frac{1}{2}cn^3 + n =$ \par
$= cn^3 - \frac{1}{2}cn^3 + n = cn^3 - (\frac{1}{2}cn^3 - n) \leq cn^3$ \par
Pokazaliśmy, że $T(n) = O(n^2)$ \par
\vspace{1\baselineskip}
Spróbujmy wzmocnić zał. indukcyjne: $T(n) \leq c_1 n^2 - c_2 n$ \par
$T(n) \leq 4T(\frac{n}{2}) + n \leq 4(c_1 (\frac{n}{2})^2 - c_2 \frac{n}{2}) + n = $ \par
$ = c_1 n^2 - 2c_2 n + n = c_1 n^2 - (2c_2 - 1)n \leq c_1 n^2 - c_2 n$ \par
Musimy dobrać takie $c_1 i c _2$, aby $2c_1 \geq c_2$ \par
Wówczas otrzymamy $T(1) = O(1) \leq c_1 1^2 - c_2 1$ \par
\vspace{9\baselineskip}
Przykład 2: \par
\vspace{1\baselineskip}
$T(n) = 2T(\sqrt{n}) + log(n)$ \par
Załóżmy, że n jest potęgą dwójki $n = 2^m \rightarrow m = log(n)$ \par
$T(2^m) = 2T(2^{m/2}) + m$ \par
oznaczmy $T(2^m) = S(m)$ \par
$T(2^m) = 2T(2^{m/2}) + m \rightarrow 2S(m/2) + m$ \par
$S(m) = O(m log(m))$ \par
$T(n) = O(log(n) log(log(n)))$ (formalnie powinniśmy to udowodnić) \par  
\vspace{1\baselineskip}
\textbf{Drzewo rekursji} \par
\vspace{1\baselineskip}
Przykład : $T(n) = T(\frac{n}{2}) +T (\frac{n}{4}) + n^2$ \par
\vspace{1\baselineskip}
\begin{center}
    \begin{tikzpicture}
        [level distance=1.5cm,
        level 1/.style={sibling distance=4cm},
        level 2/.style={sibling distance=2cm}]
        \node {$n^2$}
            child {node {$\frac{n^2}{4}$}
                child {node {$\frac{n^2}{16}$}}
                child {node {$\frac{n^2}{64}$}}
            }
            child {node {$\frac{n^2}{16}$}
                child {node {$\frac{n^2}{64}$}}
                child {node {$\frac{n^2}{256}$}}
            };
          \node[draw=none] at (-4.5,0) {$n^2$};
          \node[draw=none] at (-4.5,-1.5) {$\frac{5}{16}n^2$};
          \node[draw=none] at (-4.5,-3) {$\frac{25}{256}n^2$};
    \end{tikzpicture}
\end{center}
\vspace{1\baselineskip}
Trzeba pamiętać, że drzewo rekursji samo w sobie nie jest formalnym rozwiązaniem problemu. Nie można go urzywać do dowodzenia złożoności algorytmów.
Jest to jedynie intuicyjne podejście do problemu. Formmalnie T(n) należałoby policzyć jako sumę wszystkich wierzchołków w drzewie rekursji:
\[
    T(n) = \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k \cdot n^2 = n^2 \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k = n^2 \frac{1}{1-\frac{5}{16}} = n^2 \frac{16}{11} = \frac{16}{11}n^2
\]
Widzimy zatem, że $T(n) = O(n^2)$ \par
\vspace{1\baselineskip}
\textbf{Master Theorem} \par
\vspace{1\baselineskip}
Niech $a \geq 1, b > 1, f(n), d \in N$ oraz $f(n)$ będzie funkcją nieujemną. Rozważmy rekurencję: \par
\[
    T(n) = aT(\frac{a}{b}) + \Theta(n^d)
\] 
Wówczas: \par
\begin{itemize}
    \item $\Theta(n^d)$, jeśli $d > log_b a$
    \item $\Theta (n^d log(n))$, jeśli $d = log_b a$
    \item $\Theta(n^{log_b a})$, jeśli $d < log_b a$
\end{itemize}
Do przedstawienia problemu użyjemy drzewa rekursji. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$c \cdot n^d$}
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        }
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        };
      \node[draw=none] at (-4.5,0) {$n^d$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{n^d}{b^d}$};
      \node[draw=none] at (-4.5,-3) {$\frac{n^d}{b^{2d}}$};
  \end{tikzpicture} 
\end{center}
\begin{enumerate}
    \item suma kosztoów w $k$--tym kroku
        \[
            a^k c (\frac{n}{b^k})^d = c (\frac{a}{b^d})^k n^d
        \]
        gdzie $c(\frac{n}{b^k})^d$ to koszt jednego podproblemu w $k$--tym kroku
    \item obliczenie wysokości drzewa:
        \[
            \frac{n}{b^h} = 1 \rightarrow h = \log_b n
        \]
    \item Obliczenie $T(n)$
    \begin{align*}
        T(n) &= \Theta\left(\sum^{\log_b n}_{k=0} c\frac{a}{b^k}n^d\right) \\
             &= \Theta\left(c \cdot n^d \sum^{\log_b n}_{k=0} \left(\frac{a}{b^d}\right)^k\right) \\
             &= \Theta\left(c \cdot n^d \frac{1-\left(\frac{a}{b^d}\right)^{\log_b n + 1}}{1-\frac{a}{b^d}}\right) \\
             &\implies T(n) = \Theta(n^d)
    \end{align*}
    
    \item rozważmy 3 przypadki:
        \begin{enumerate}
            \item $d > \log_b a$ 
                \[
                    T(n) = \Theta(n^d)
                \]
            \item $d = \log_b a$ 
                \[
                    T(n) = \Theta(n^d \log n)
                \]
            \item $d < \log_b a$
                \[
                    T(n) = \Theta(n^{\log_b a})
                \]
        \end{enumerate}
\end{enumerate}

\subsubsection*{Przykłady}
\begin{itemize}
    \item $T(n) = 4T(\frac{n}{2}) + 11n$ \newline
        Wtedy kożystając z \textbf{Master Theorem} mamy:
        \[
            a = 4, b = 2, d = 1
        \]
        Jak i również
        \[
            \log_b a = \log_2 4 = 2 > 1 = d \implies T(n) = \Theta(n^2)
        \]
    \item $T(n) = 4T(\frac{n}{3}) + 3n^2$ \newline
        Wtedy
        \[
            a = 4, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 4 < 2 = d \implies T(n) = \Theta(n^2)
        \]
    \item $T(n) = 27T(\frac{n}{3}) + \frac{n^2}{3}$ \newline
        Wtedy
        \[
            a = 27, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 27 = 3 > 2 = d \implies T(n) = \Theta(n^3\log n)
        \]
\end{itemize}

\subsection*{Metoda dziel i zwyciężaj (D\&C)}
Na czym ona polega?
\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy 
    \item Rozwiazanie rekurencyjnie mniejsze podpoblemy
    \item połącz rozwiązania podproblemów w celu rozwiązania problemu wejściowego
\end{enumerate}
\subsubsection*{Algorytm -- Binary Search}
\begin{itemize}
    \item \textbf{Input}: posortowania tablica \texttt{A[1..n]} oraz element \texttt{x}
    \item \textbf{Output}: indeks \texttt{i} taki, że \texttt{A[i] = x} lub \texttt{0} jeśli \texttt{x} nie występuje w \texttt{A}
        \item przebieg algorytmu: 
            \begin{algorithm}[H]
                \caption{Binary Search}
                \begin{algorithmic}[1]
                    \Procedure{BinarySearch}{A, x}
                        \State $l = 1$
                        \State $r = |A|$
                        \While{$l \leq r$}
                            \State $m = \lfloor \frac{l+r}{2} \rfloor$
                            \If{$A[m] = x$}
                                \State \Return{$m$}
                            \ElsIf{$A[m] < x$}
                                \State $l = m + 1$
                            \Else
                                \State $r = m - 1$
                            \EndIf
                        \EndWhile
                        \State \Return{0}
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}
        \item \textbf{Asypmtotyka}
            Algorytm spełnia następująca rekurencje:
            \[
                T(n) = T(\frac{n}{2}) + \Theta(1)
            \]
            Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
            \[
                T(n) = \Theta(\log n)
            \]
\end{itemize}
\vspace{1\baselineskip}
\subsubsection*{Divide \& Conquer}

\textbf{Problem:} Obliczenie $x^n$.  

Rozwiązanie naiwną metodą iteracyjną:  
\[
x^n = x \cdot x \cdot \dots \cdot x \quad \Rightarrow \quad \Theta(n)
\]

Rozwiązanie za pomocą Divide \& Conquer:  

\[
x^n =
\begin{cases}
    (x^{\frac{n}{2}}) \cdot (x^{\frac{n}{2}}), & \text{gdy } n \text{ jest parzyste} \\
    (x^{\frac{n-1}{2}}) \cdot (x^{\frac{n-1}{2}}) \cdot x, & \text{gdy } n \text{ jest nieparzyste}
\end{cases}
\]

Rekurencyjna złożoność czasowa:
\[
T(n) = T(n/2) + \Theta(1) = \Theta(\log n)
\]

---

\textbf{Problem:} Obliczenie $n$-tej liczby Fibonacciego  

Metoda rekurencyjna:
\[
F(n) = F(n-1) + F(n-2)
\]
Ma ona złożoność wykładniczą:
\[
\Theta (\phi^n), \quad \text{gdzie } \phi = \frac{1 + \sqrt{5}}{2}
\]

Drzewo rekurencyjne dla $F_4$:

\begin{center}
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=6cm},
    level 2/.style={sibling distance=3cm},
    level 3/.style={sibling distance=1.5cm}]
    
  \node {$F_4$}
      child {node {$F_3$}
          child {node {$F_2$}
              child {node {$F_1$}}
              child {node {$F_0$}}
          }
          child {node {$F_1$}}
      }
      child {node {$F_2$}
          child {node {$F_1$}}
          child {node {$F_0$}}
      };

\end{tikzpicture}
\end{center}

\textbf{Wzór jawny:}
\[
F_n = \frac{1}{\sqrt{5}} \left( \phi^n - (-\phi)^{-n} \right)
\]


Obliczanie $F_n$ macierzą:
Zamiast rekurencji można użyć potęgowania macierzy, co daje optymalną złożoność.  
Dla każdego $n \geq 0$ zachodzi:

\[
\begin{bmatrix}
    1 & 1 \\
    1 & 0
\end{bmatrix}^n
=
\begin{bmatrix}
    F_{n+1} & F_n \\
    F_n & F_{n-1}
\end{bmatrix}
\]

Potęgowanie macierzy metodą szybkiego potęgowania daje czas:
\[
\Theta(\log n)
\]
co jest znaczną poprawą w porównaniu do wykładniczej rekurencji.

\textbf{Mnożenie liczb binarnych metodą Divide \& Conquer}

\textbf{Wejście:} $x, y$  
\textbf{Wyjście:} $x \cdot y$

Każdą liczbę można rozbić na dwie połowy:
\[
x = x_L \cdot 2^{\frac{n}{2}} + x_R
\]
\[
y = y_L \cdot 2^{\frac{n}{2}} + y_R
\]

Podstawiając do iloczynu:
\[
xy = (x_L \cdot 2^{\frac{n}{2}} + x_R) \cdot (y_L \cdot 2^{\frac{n}{2}} + y_R)
\]

Po rozwinięciu:
\[
xy = x_L y_L \cdot 2^n + (x_L y_R + x_R y_L) \cdot 2^{\frac{n}{2}} + x_R y_R
\]

Rekurencyjna zależność czasowa:
\[
T(n) = 4T(n/2) + \Theta(n)
\]

Zastosowanie \textbf{Master Theorem} daje:
\[
T(n) = \Theta(n^2)
\]
co pokazuje, że metoda ta nie poprawia złożoności względem standardowego mnożenia. 

\vspace{1\baselineskip}
\textbf{Optymalizacja: metoda Gaussa}

Zamiast wykonywać 4 mnożenia rekursywne, można zastosować zasadę Gaussa:
\[
xy = x_L y_L \cdot 2^n + ((x_L + x_R)(y_L + y_R) - x_L y_L - x_R y_R) \cdot 2^{\frac{n}{2}} + x_R y_R
\]

Dzięki temu zamiast 4 mnożeń wykonujemy tylko 3:
\[
T(n) = 3T(\frac{n}{2}) + \Theta(n)
\]

Zastosowanie \textbf{Master Theorem} daje:
\[
T(n) = \Theta(n^{\log_2 3})
\]

\begin{algorithm}[H]
    \caption{Multiply - Mnożenie dużych liczb binarnych metodą Gaussa}
    \label{alg:multiply}
    \begin{algorithmic}[1]
        \Procedure{multiply}{x, y}
            \State $n \gets \max(|x|, |y|)$ 
            \If{$n = 1$} 
                \State \Return{$x \cdot y$}
            \EndIf
            \State $m \gets \lceil {n/2} \rceil$
            \State $x_L, x_R \gets$ 
            \State $y_L, y_R \gets$ 
            \State $p_1 \gets \Call{multiply}{x_L, y_L}$
            \State $p_2 \gets \Call{multiply}{x_R, y_R}$
            \State $p_3 \gets \Call{multiply}{(x_L + x_R), (y_L + y_R)}$
            \State \Return{$p_1 \cdot 2^{2m} + (p_3 - p_1 - p_2) \cdot 2^m + p_2$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\textbf{QuickSort}
\begin{algorithm}[H]
    \caption{QuickSort - Sortowanie szybkie}
    \label{alg:quicksort}
    \begin{algorithmic}[1]
        \Procedure{quicksort}{A, low, high}
            \If{$low < high$}
                \State $p \gets \Call{partition}{A, low, high}$
                \State \Call{quicksort}{A, low, p - 1}
                \State \Call{quicksort}{A, p + 1, high}
            \EndIf
        \EndProcedure
        \\
        \Procedure{partition}{A, low, high}
            \State $pivot \gets A[high]$
            \State $i \gets low - 1$
            \For{$j \gets low$ \textbf{to} $high - 1$}
                \If{$A[j] \leq pivot$}
                    \State $i \gets i + 1$
                    \State \Call{swap}{A[i], A[j]}
                \EndIf
            \EndFor
            \State \Call{swap}{A[i + 1], A[high]}
            \State \Return{$i + 1$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/quick.png}
    \label{fig:example_image}
\end{figure}

\begin{algorithm}[H]
    \caption{Hoare Partition}
    \label{alg:quicksort}
    \begin{algorithmic}[1]
        \Procedure{hoare\_partition}{A, p, q}
            \State $pivot \gets A\left[\left\lfloor \frac{p + q}{2} \right\rfloor\right]$
            \State $i \gets p - 1$
            \State $j \gets q + 1$
            \While{true}
                \Repeat
                    \State $i \gets i + 1$
                \Until{$A[i] \geq pivot$}
                \Repeat
                    \State $j \gets j - 1$
                \Until{$A[j] \leq pivot$}
                \If{$i \geq j$}
                    \State \Return $j$
                \EndIf
                \State swap($A[i], A[j]$)
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\par\textbf{Analiza worst-case QuickSorta}

\par
$T(n) = T(n - 1) + T(0) + \Theta(n) = T(n - 1) + \Theta(n)$

\par
Drzewo rekurencji (dla przypadku pesymistycznego, tj. jednostronny podział):

\begin{center}
\begin{tikzpicture}[
    level distance=1.4cm,
    every node/.style={circle,draw},
    level 1/.style={sibling distance=5cm},
    level 2/.style={sibling distance=3cm},
    level 3/.style={sibling distance=2cm},
    level 4/.style={sibling distance=1.5cm}
    ]
\node {$c_n$}
  child {node {$c_{n-1}$}
    child {node {$c_{n-2}$}
      child {node {$c_{n-3}$}
        child {node {$\cdots$}
          child {node {$c_1$}
            child[missing]
            child[missing]
          }
          child[missing]
        }
        child[missing]
      }
      child[missing]
    }
    child[missing]
  }
  child {node {$\Theta(1)$}}; % prawa strona (najmniejsza partycja)
\end{tikzpicture}
\end{center}


\par
$T(n) \leq \sum_{i=1}^{n} c \cdot i = c \cdot \sum_{i=1}^{n} i = \Theta(n^2)$
\vspace{1\baselineskip}

\par\textbf{Analiza best-case}

\par
Jeśli pivot zawsze dzieli tablicę na dwie równe części:

\[
T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n)
\Rightarrow T(n) = \Theta(n \log n)
\]

\par\textbf{Analiza average-case}

\par
Niech $T_n$ oznacza liczbę porównań dla tablicy długości $n$.

\[
x_k =
\begin{cases}
    1, & \text{jeśli partition dzieli tablicę na } (k,\; n - k - 1) \\[0.5em]
    0, & \text{w przeciwnym wypadku}
\end{cases}
\]


\[
T_n = \sum_{k=0}^{n-1} x_k \cdot (T_k + T_{n-k-1}) + (n - 1)
\]

\par
Liczymy wartość oczekiwaną:

\[
E(T_n) = \sum_{k=0}^{n-1} \mathbb{E}(x_k) \cdot \left( \mathbb{E}(T_k) + \mathbb{E}(T_{n-k-1}) \right) + (n - 1)
\]

\[
\mathbb{E}(x_k) = \frac{1}{n} \quad \text{(bo pivot jest losowy)}
\]

\[
E(T_n) = \frac{1}{n} \sum_{k=0}^{n-1} \left( E(T_k) + E(T_{n-k-1}) \right) + (n - 1)
\]

\[
= \frac{2}{n} \sum_{k=0}^{n-1} E(T_k) + (n - 1)
\]

\[
\Rightarrow E(T_n) = \Theta(n \log n)
\]

    \vspace{1\baselineskip}
    \textbf{Analiza avg Case'a}
    $T_n \rightarrow$ Liczba porównań elementów sortowanej tablicy: |A| = n \newline

\[
x_k =
\begin{cases}
    1, & \mbox{jeśli partition dzieli tablicę na } (k,\, n - k - 1) \\[0.6em]
    0, & \mbox{w przeciwnym wypadku}
\end{cases}
\]


\[
    T_n =
    \begin{cases}
        T_0 + T_{n-1} + n-1, gdy (0, n-1) -split \\
        T_1 + T_{n-2} + n -1, gdy (1, n-2) -split \\
        ... \\
        T_k + T_{n-1-k} + n - 1, gdy (k, n-k-1) -split \\
        ... \\
        T_{n-1} + T_0 + n - 1, gdy (n-1, o) -split
    \end{cases}
    \]


    $T_n = \sum_{k=0}^{n-1} x_k (T_k + T_{n-k-1}) + n - 1$ \newline
    
    liczymy wartosć oczekiwaną: \newline

    $E(T_n) = E (\sum{k=0}^{n-1} X_k (T_k + T_{n-k-1} + n - 1))$ \newline
    $E(T_n) = \sum_{k=0}^{n-1} E(X_k \cdot (T_k + T_{n-k-1}) + n - 1) $ \newline
    $E(T_n) = \sum_{k=0}^{n-1} E(X_k) - E(T_k + T_{n-k-1} - n - 1)$ \newline
    $E(T_n) = \frac{1}{n} \cdot \sum_{k=0}^{n-1} E(T_k) + \sum (E(T_{n-k-1}))$ \newline


    \textbf{Dual pivot quicksort} \newline

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/dpq.png}
        \label{fig:example_image}
    \end{figure}

    \textbf{Wartość oczekiwana: } 

    $E(\mbox{liczba porównań w dual pivot partition}) \approx \frac{16}{9}n$ 

    $E(\mbox{liczba porównań w dual pivot qs sedwick}) \approx \frac{32}{15}n logn$  \newline


    \textbf{Yaroslavsky dual pivot qs} 

    $E(\mbox{liczba porównań w partition}) \approx \frac{19}{12}n$ 

    $E(\mbox{liczba porównań w Dual Pivot qs Yaroslavsky}) \approx 1.9 n logn$ \newline

    \textbf{Strategia count}

    $E(\mbox{liczba porównań w Count Partition}) \approx \frac{3}{2}n$

    $E(\mbox{liczba porównań w Dual Pivot qs z count}) \approx 1.8 n logn$ \newline

    \textbf{Comparsion Model} 

    Dolne ograniczenie na liczbę porównań w problemie sortowania \par
    w Comparsion Model wynosi $\Omega(n logn)$ \newline

    D-d: \par
    \begin{itemize}
        \item dla dowolnego algorytmu sortującego możemy znależć odpowiadające mu drzewo decyzyjne
        \item n! liści w binarnym drzewie decyzyjnym
        \item drzewo binarne pełne o wysokości h ma co najmniej $2^h$ liści
        \item ale liści w drzewie decyzyjnym powinno być co najmniej n!, zatem: \par
        $2^h \leq n!$ / lg \par
        $h \leq \log_{2}n!$ \par
        $lg n! = lg(\sqrt{s \pi n} (\frac{n}{e})^n (1 + o(1)))$ \par
        $lg (\frac{n}{e})^n + lg (\sqrt(2 \pi n)(1 + o(1)))$ \par
        $n logn - n lg e + lg(\sqrt{2 \pi n} (2 + o(1))) = \Omega(n logn)$
    \end{itemize}    \par

    Sortowanie: \par
    Input: $|a| = n, \forall i \in \{1, ..., k\}$ \par
    Output: posortowana rosnąco tablica A \par

    \begin{algorithm}[H]
        \caption{CountingSort}
        \label{alg:countingsort}
        \begin{algorithmic}[1]
            \Procedure{counting\_sort}{A, n, k}
                \For{$i = 1$ \textbf{to} $k$}
                    \State $C[i] \gets 0$
                \EndFor
                \For{$i = 1$ \textbf{to} $n$}
                    \State $C[A[i]] \gets C[A[i]] + 1$
                \EndFor
                \For{$i = 2$ \textbf{to} $k$}
                    \State $C[i] \gets C[i] + C[i - 1]$
                \EndFor
                \For{$i = n$ \textbf{downto} $1$}
                    \State $B[C[A[i]]] \gets A[i]$
                    \State $C[A[i]] \gets C[A[i]] - 1$
                \EndFor
                \State \Return $B$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm} \par

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/cs.png}
        \label{fig:example_image}
    \end{figure} \par

    Złożoność obliczeniowa Counting Sorta: \par
    $\Theta (n + k)$ gdzie $k = O(n)$ \par

    \vspace{1\baselineskip}

    \textbf{Stable Sorting Property} \par

    Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej

    \vspace{5\baselineskip}

    \textbf{RadixSort} \par

    \begin{algorithm}[H]
        \caption{RadixSort}
        \label{alg:radixsort}
        \begin{algorithmic}[1]
            \Procedure{radix\_sort}{A, n, d}
                \For{$i = 1$ \textbf{to} $d$}
                    \State $counting\_sort(A, n, 9)$
                \EndFor
                \State \Return $A$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm} \par
    \vspace{1\baselineskip}

    \textbf{Złożoność obliczeniowa RadixSorta} \par
    \begin{itemize}
        \item n liczb b'bitowych
        \item liczb b bitowych dzielimy na r-bitowe cyfry
        \item cyfry są z |{$0,...,2^n-1$}| = $2^n$ \par
        \item Counting Sort sortujący n liczb względem jednej cyfry
    \end{itemize} \par
    Zatem RadixSort będzie miał złożoność obliczneiową: \par
    $\Theta(\frac{b}{r} \cdot (n + 2^r))$ \par
    Co po wykonaniu skomplikowanej analizy daje: \par
    $\Theta(d \cdot n)$ \par

    \vspace{2\baselineskip}
    \textbf{Statystyki pozycyjne} \par
    \vspace{1\baselineskip} \par
    \textbf{Def: } k-tą statystyką pozycyjną nazywam← k-tą najmniejszą wartość z \par
    zadanego zbioru \par
    przykład: \par
    \begin{itemize}
        \item $k=1 \rightarrow O(n)$
        \item $k=n$ $\rightarrow O(n)$
        \item $k=\lfloor \rightarrow \text { sortowanie } O(n \log n)$
    \end{itemize}
    \vspace{1\baselineskip}
    \begin{algorithm}[H]
        \caption{RandomSelect}
        \label{alg:randomselect}
        \begin{algorithmic}[1]
            \Procedure{random\_select}{A, p, q, i}
                \If{$p = q$}
                    \State \Return $A[p]$
                \EndIf
                \State $r \gets$ RandPartition($A$, $p$, $q$)
                \State $k \gets r - p + 1$
                \If{$i = k$}
                    \State \Return $A[r]$
                \ElsIf{$i < k$}   
                    \State \Return \Call{random\_select}{$A$, $p$, $r - 1$, $i$}
                \Else
                    \State \Return \Call{random\_select}{$A$, $r + 1$, $q$, $i - k$}
                \EndIf         
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    
    \vspace{1\baselineskip}
    \textbf{Select algorithm} \par
    \begin{itemize}
        \item dzielimy A[p..q] na $\frac{n}{\lfloor 5 \rfloor}$ pięcioelementowych częsci
        oraz ostanią część na $\leq$ 5 elementów
        \item Sortujemy te grupy i wybieramy z każdej z nich medianę
        \item Znajdujemy medianę M. Select(M, 1, $\frac{n}{5}$, $\frac{n}{10}$)
        \item Ustalamy X jako pivot; Partition(A, p, q) i tak samo jak w RandomSelect
    \end{itemize}

    \textbf{Select} \par
    Select(A, K) $\rightarrow$ T(n) \par
    \begin{itemize}
        \item Dziel na 5 elementowe tablice i znajdź ich medianę $\rightarrow \Theta(n)$
        \item Select (...) $\rightarrow$ znajdź medianę median $\rightarrow T(\lceil{\frac{n}{5}}\lceil)$ \par
        \item Użyj mediany median jako pivot w Partition $\rightarrow \Theta(n)$
        \item Idź do lewej albo prawej podtablicy w zależności od indeksu pivota i szukaj statystyki pozycyjnej
    \end{itemize}
    Otrzymujemy: $t(n) = T(\lceil{\frac{n}{5}}\lceil) + \Theta(?)$ \par
    \vspace{9\baselineskip}
    
    \textbf{Struktury danych} \par

    Set interface: \par
    \begin{itemize}
        \item build (A) - buduje set z danych zawartych w A
        \item length - zwraca moc zbioru
        \item find (k) - zwraca element zbioru o kluczu równym k
        \item insert (k) - dodaje element o kluczu k do zbioru
        \item delete (k) - usuwa element o kluczu k ze zbioru
        \item find\_max - zwróc element o największym kluczu
        \item find\_min - zwróć element o najmniejszym kluczu
        \item find\_prev - zwraca element poprzedni od klucza
    \end{itemize}
    \newpage 
    \vspace*{\fill}
    \begin{table}[H]
    \centering
    \begin{tabular}{lcccccc}
    \toprule
    \textbf{Struct} & \textbf{build} & \textbf{find} & \textbf{insert / delete} & \textbf{find min / find max} & \textbf{find\_prev} & \textbf{sort} \\
    \midrule
    unsorted array       & $\Theta(n)$         & $\Theta(n)$         & $\Theta(n)$               & $\Theta(n)$              & $\Theta(n)$          & $\Theta(n \log n)$ \\
    sorted array         & $\Theta(n \log n)$  & $\Theta(\log n)$    & $\Theta(n)$               & $\Theta(1)$              & $\Theta(\log n)$     & $\Theta(n)$ \\
    pointers list & $\Theta(n)$         & $\Theta(n)$         & $\Theta(1)\ /\ \Theta(n)$ & $\Theta(n)$              & $\Theta(n)$          & $\Theta(n \log n)$ \\
    DAA                  & $\Theta(n)$         & $\Theta(1)$         & $\Theta(1)$               & $\Theta(n)$              & $\Theta(n)$          & $\Theta(n)$ \\
    BST                  & $\Theta(n)$         &                     &                            &                          &                      & \\
    \bottomrule
    \end{tabular}
    \end{table}
    \vspace*{\fill}
    \newpage
    \vspace{1\baselineskip}
    \textbf{Binary Search Tree} \par
    BST property : \par
    \begin{itemize}
        \item x $\in$ T - x jest węzłem drzewa T
        \item Wówczas każdy y $in$ x.left ma y.key < x.key
        \item key y $in$ x.right ma y.key > x.key
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Inorder Tree Walk} \par
    \begin{algorithm}[H]
        \caption{Inorder Tree Walk}\label{alg:inorder_tree_walk}
        \begin{algorithmic}[1]
        \Procedure{InorderTreeWalk}{x $\in$ T}
            \If{$x \neq \text{null}$}
                \State \Call{InorderTreeWalk}{x.left}
                \State print(x)
                \State \Call{InorderTreeWalk}{x.right}
            \EndIf
        \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \newpage
    \textbf{Tree Search} \par
    \begin{algorithm}[H]
        \caption{TreeSearch}\label{alg:tree_search}
        \begin{algorithmic}[1]
        \Procedure{TreeSearch}{x $\in$ T, k}
            \If{$x = \text{null} \lor k = x.key$}
                \State \Return x
            \ElsIf{$k < x.key$}
                \State \Return \Call{TreeSearch}{x.left, k}
            \Else
                \State \Return \Call{TreeSearch}{x.right, k}
            \EndIf
        \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \vspace{1\baselineskip}
    \textbf{BST - Delete} \par
    \begin{itemize}
        \item x jest liścciem - zwolnij pamięć zajmowaną przez x, wstaw wskaźnik na jego ojca (na niego / na null'a)
        \item x ma jedno poddrzewo - x ma syna v to: \par
        \begin {itemize}
            \item zwalniamy pamięć x
            \item ojciec x wskazuje na v
            \item v.p wskazuje na x.p
        \end{itemize}
        \item x ma dwa poddrzewa: \par
        \begin{itemize}
            \item znajdź następnika x -> y
            \item zastąp dane x danymi z y
            \item skasuj y
        \end{itemize}
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Twierdzenie: } Niech T będzie losowym drzewem  BST o n-węzłach. wtedy: \par
    
    $E(h(t)) \leq 3log_2{n} = o(logn)$ \par

    D-d: \par
    Nierówność Jensena: f-wypukła: \par
    $f(E(x)) \leq E(f(x))$

    Zamiast analizować zmienną losową h(t) będziemy się zajmować zmienną losową $H_{n}$, będziemy się zajmować $Y_{n} = 2^{H_{n}}$ \par

    Pokażemy, że $E(Y_{n}) = O(n^3)$ \par

    $2^{H_{n}} \leq E(2^{H_{n}}) = E(Y_{n}) = O(n^3) //log_{2}$ \par
    $E(H_{n}) = 3 \cdot log_2{n} + o(lnn)$ \par
    \newpage
    \textbf{Drzewa czerwono-czarne} \par   
    \begin{itemize}
        \item Drzewo czerwono-czarne jest drzewem BST
        \item Każdy węzeł jest czerwony albo czarny
        \item Korzeń oraz liście są czarne
        \item Czerwony węzeł nie może mieć czerwonego ojca
        \item Każda ścieżka od węzła do liścia ma tę samą liczbę czarnych węzłów (ścieżkę tę będziemy nazywać black-height i oznaczać jako bh(x))
    \end{itemize}
    \vspace{1\baselineskip}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/rbt.png}
        \label{fig:example_image}
    \end{figure} \par
    \vspace{1\baselineskip}
    \textbf{Lemat: } Niech T będzie drzewem czerwono-czarnym o n węzłach. \par Wówczas wysokość drzewa T jest z góry ograniczona przez: \par
    \begin{center}
        $\text{wysokość}(T) \leq 2 \cdot log_2(n + 1)$
    \end{center}
    \vspace{1\baselineskip}
    \textbf{RB - Insert} \par
    \begin{itemize}
        \item Wstawiamy węzeł z w taki sposób jak w BST
        \item z.kolor = czerwony
        \item FixUp (nie chodzi o zespół punkowy)
    \end{itemize}
    Więcej o drzewacz czerwono - czarnych można znaleźć pod linkiem: \par 
    \begin{center}
        https://inf.ug.edu.pl/~pmp/Z/ASDwyklad/czczWUd.pdf
    \end{center} \par
    \newpage
    \textbf{Directed Access Array} \par
    \begin{itemize}
        \item klucze należą do {0, ..., k - 1}, k = moc zbioru kluczy
        \item klucze będziemy utożsamiać z adresami w pamięci komputera
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Hash Tables} \par
    \vspace{1\baselineskip}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/ht.png}
        \label{fig:example_image}
    \end{figure} \par
    Ponieważ k >> m (k - zbiór kluczy, m - wartości hash): funkcja hash  \par nie będzie różnowartościowa:
    \begin{center}
        $\exists k_1, k_2: h(k_1) = h(k_2)$
    \end{center}
    \vspace{1\baselineskip}
    Aby poradzić sobie z tym problemem:
    \begin{itemize}
        \item open adressing
    \end{itemize}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/oa.png}
        \label{fig:example_image}
    \end{figure} \par
    Chcemy abu funkcja h(hash) zwracała wyniki z rozkładu jednostajnego na zbiorze {0, ..., m-1}
    \begin{itemize}
        \item Division hash function: h(k) = k mod m
        \item Universal hash function: $H_{a,b}(k) = ((a \cdot k + b)\text{mod p}) \text{mod m}$
            \begin{center}
                $H(p, m) = {h_{a,b}: a, b \in {0, ..., p-1}, a \neq 0}$
            \end{center}
    \end{itemize}

    Własności Universal hash function: \par
    \begin{itemize}
        \item Universal hash property:
            \begin{center}
                $P_{h \in H} (h(k_i) = h(k_j)) \leq \frac{1}{m}, \forall k_1, k_j \in  \text{\{0, ..., k-1\}}$
            \end{center}
            D - d: \par
            zakładamy, że $h_{a, b}$ ma Universal Hash Property. \par
            $a \cdot x + b \equiv a \cdot y + b + i \cdot m \text {mod p}$ \par
            Skoro p > k, $x \neq y \rightarrow x - y \neq 0$ oraz ma odwrotność mod p. \par
            $a \equiv i \cdot m \cdot (x-y)^{-1} \text{mod p}$ \par
            zatem otrzymujemy: \par
            $P_r \text{(kolizja)} \leq \frac{\lfloor \frac{p-1}{m} \rfloor}{p-1} \leq \frac{\frac{p-1}{m}}{p-1} = \frac{1}{m}$
            \vspace{1\baselineskip}
            Wartość oczekiwana kolizji jest rzędu $\Theta(1)$
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Wzbogacanie strukur danych} \par
    \begin{itemize}
        \item Ex - dynamiczne statystyki pozycyjne
        \begin{itemize}
            \item dynamiczna strukura danych
            \item OS-Select(i) - zwróci i-tą statystykę pozycyjną z danych
            \item OS-Rank(x) - numer statystyki pozycyjnej X w danych
            
            \begin{algorithm}[H]
                \caption{OS-Select}\label{alg:os-select}
                \begin{algorithmic}[1]
                    \Procedure{OS-Select}{x, i}
                        \State $k \gets \text{size}(x.\text{left}) + 1$
                        \If{$i = k$}
                            \State \Return $x$
                        \ElsIf{$i < k$}
                            \State \Return \Call{OS-Select}{$x.\text{left}, i$}
                        \Else
                            \State \Return \Call{OS-Select}{$x.\text{right}, i - k$}
                        \EndIf
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}
            
            \begin{algorithm}[H]
                \caption{OS-Rank}\label{alg:os-rank}
                \begin{algorithmic}[1]
                    \Procedure{OS-Rank}{x}
                        \State $r \gets \text{size}(x.\text{left}) + 1$
                        \State $y \gets x$
                        \While{$y \neq \text{root}$}
                            \If{$y = y.\text{parent.right}$}
                                \State $r \gets r + \text{size}(y.\text{parent.left}) + 1$
                            \EndIf
                            \State $y \gets y.\text{parent}$
                        \EndWhile
                        \State \Return $r$
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}
        \end{itemize}
        \item Metodologia wzbogacenia struktur danych
        \begin{itemize}
            \item Wybierz strukturę
            \item Wybrać dodatkową informację, która ma być przechowywana w strukturze
            \item Upewnić się, że złożoność obliczeniowa operacji na strukturze danych nie ulegnie pogorszeniu
            \item Zaprojektować dodatkowe operacje na strukturze danych, wykorzystującą dodatkową informację
        \end{itemize}
    \end{itemize}

    \vspace{1\baselineskip}
    \textbf{Drzewa przedziałowe} \par

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/dp.png}
        \label{fig:example_image}
    \end{figure} \par

    Własności: \par
    \begin{itemize}
        \item Przechowuje punkty oraz wszystkie przedziały związane z tymi punktami
        \item Jest zrównoważone
        \item Odpowiada na pytania dotyczące przedziałów w czasie logarytmicznym
    \end{itemize}

    \begin{algorithm}[H]
        \caption{IntervalSearch}\label{alg:interval-search}
        \begin{algorithmic}[1]
            \Procedure{IntervalSearch}{$i, T$}
                \State $x \gets \text{root}(T)$
                \While{$x \neq \text{null}$ \textbf{and} $(i.\text{low} > x.\text{high} \ \textbf{or} \ x.\text{low} > i.\text{high})$}
                    \If{$x.\text{left} \neq \text{null}$ \textbf{and} $i.\text{low} < x.\text{left}.m$}
                        \State $x \gets x.\text{left}$
                    \Else
                        \State $x \gets x.\text{right}$
                    \EndIf
                \EndWhile
                \State \Return $x$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \newpage
    \textbf{Programowanie dynamiczne} \par
    Programowanie dynamiczne to technika projektowania algorytmów \par
    polegająca na rozwiązywaniu podproblemów i zapamiętywaniu ich wyników \par
    Przykład: n-ta liczba Fibonacciego (memoizacja): \par
    \begin{algorithm}[H]
        \caption{FibonacciMemo}\label{alg:fibonacci-memo}
        \begin{algorithmic}[1]
            \Procedure{FibonacciMemo}{$n, memo$}
                \If{$n \leq 1$}
                    \State \Return $n$
                \EndIf
                \If{$memo[n] \neq -1$}
                    \State \Return $memo[n]$
                \EndIf
                \State $memo[n] \gets \Call{FibonacciMemo}{n - 1, memo} + \Call{FibonacciMemo}{n - 2, memo}$
                \State \Return $memo[n]$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \vspace{1\baselineskip}
    \textbf{Problem wydawania reszty} \par
    Dane: $c_1 < c_2 < ... < c_k$ - zbiór nominałów $\in \mathbb{N}$ \par
    Szukamy minimalnej liczby monet, które sumują się do kwoty $n$ \par
    Niech L(i) będzie minimalną liczbą monet dla reszty i \par
    $L(i) = 1 + min_{1 \leq j \leq n} \{L(i - c_j): c_j \leq i\}$ \par

    \vspace{1\baselineskip}
    \textbf{Problem plecakowy} \par
    Dane: $n$ przedmiotów, $w_i$ - waga przedmiotu i, $v_i$ - wartość przedmiotu i \par
    Ograniczenie górne na pojemność plecaka: $W$ \par
    Szukamy maksymalnej wartości przedmiotów, które zmieszczą się w plecaku: 
    $\mathbb{I} \in \{1, .., n\}$ takie, że:
    \begin{itemize}
        \item $\sum_{i \in \mathbb{I}} w_i \leq W$
        \item $\sum_{i \in \mathbb{I}} v_i$ jest maksymalne
    \end{itemize}
    \begin{algorithm}[H]
        \caption{Algorytm plecakowy}\label{alg:knapsack}
        \begin{algorithmic}[1]
            \Procedure{Knapsack}{$W, w, c, n$}
                \State $\text{utwórz tablicę } A[0 \dots W]$
                \For{$i \gets 0 \text{ to } W$}
                    \State $A[i] \gets 0$
                \EndFor
    
                \For{$i \gets 0 \text{ to } W$}
                    \For{$j \gets 1 \text{ to } n$}
                        \If{$w[j] \leq i$} 
                            \State $A[i] \gets \max\left(A[i],\, A[i - w[j]] + c[j]\right)$
                        \EndIf
                    \EndFor
                \EndFor
                \State \Return $A[W]$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \newpage
    \textbf{Grafy jako struktura danych} \par
    \vspace{1\baselineskip}
    Grafy w informatyce to nieliniowa struktura danych, składająca się z wierzchołków (węzłów) \par
     i krawędzi, które je łączą. Modelują relacje między obiektami, jak np. sieci społecznościowe, \par sieci komputerowe czy mapy drogowe. 
     \par Krawędzie mogą być skierowane (grafy skierowane) lub nieskierowane (grafy nieskierowane). \par
     \vspace{1\baselineskip}
     \begin{figure}[H]
        \centering
        \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/dag.png}
        \label{fig:example_image}
    \end{figure} \par
    \vspace{1\baselineskip}
    W grafach możemy wyróżnić pewne węzły na podstawie ich krawędzi. \par
    Jeżeli do węzła $u$ nie wchodzą żadne krawędzie, to nazywamy go źródłem. \par
    Jeżeli z węzła $u$ nie wychodzą żadne krawędzie, to nazywamy go ujściem. \par
    Liczenie najkrótszej ścieżki w DAG (skierowany graf acykliczny) \par
    można wykonać w czasie $O(V + E)$, gdzie $V$ to liczba węzłów, a $E$ to liczba krawędzi. \par
    \[
        L(A) = \max \begin{cases}
            L(S)+w(S, A) \\
            L(C)+w(C, A)
        \end{cases}
    \] 
    \newpage
    \vspace{1\baselineskip}
    W pseudokodzie można to zaprezentować w następujący sposób: \par
    \begin{center} 
        \begin{algorithm}
            \caption{Najkrótsza ścieżka w DAG}
            \begin{algorithmic}[1]
                \Procedure{ShortestPath}{G, E}

    % --- oznaczenie sekcji inicjalizacji ---
                \Statex \Comment{\textbf{Inicjalizacja:} \(\Theta(|V|)\)}
                \For {$v \in V$}
                \State $L(v) \gets \infty$
                \EndFor
                \State $L(S) \gets 0$
    % --- dalsza część algorytmu ---
                \Statex \Comment{\textbf{Meat algorytmu:} \(\Theta(|E|)\)}
                \For {$v \in V\setminus\{S\}$}
                \State $L(v) \gets \min_{(u,v) \in E}\{L(u)+w(u,v)\}$
                \EndFor
                \Statex
                \State \textbf{return} $L$
                \EndProcedure
            \end{algorithmic}
        \end{algorithm}
    \end{center}
    Całkowita złożoność algorytmu to $\Theta(|V|+|E|)$, ponieważ w najgorszym przypadku musimy przejść przez wszystkie węzły i krawędzie.
    \vspace{1\baselineskip}
    \textbf{Edit Distance} \par
    \vspace{1\baselineskip}
    Przykładem wykorzystania tego problemu jest użycie go w spellcheckerach. \par
    Sugerują one poprawki do błędnie napisanych słów. \par
    \begin{itemize}
        \item \textbf{Input}: $w_1, w_2$ -- słowa, $\Sigma$ -- alfabet
        \item \textbf{Output}: $EditDistance(w_1, w_2)$ -- minimalna liczba operacji potrzebnych do przekształcenia $w_1$ w $w_2$
        \item \textbf{Problem}: Znaleść minimalną liczbę operacji potrzebnych do przekształcenia $w_1$ w $w_2$
    \end{itemize}
    Zobaczmy to najpierw na \textbf{przykładzie}:
    %w_1=SNOWY
    %w_2=SNOW
    \[
    w_1=\text{SNOWY},\quad w_2=\text{SUNNY}.
    \]
    \[
    \begin{array}{c|cccccc}
    d_{i,j} &   & S & U & N & N & Y \\\hline
            & 0 & 1 & 2 & 3 & 4 & 5 \\
    S       & 1 & 0 & 1 & 2 & 3 & 4 \\
    N       & 2 & 1 & 1 & 1 & 2 & 3 \\
    O       & 3 & 2 & 2 & 2 & 2 & 3 \\
    W       & 4 & 3 & 3 & 3 & 3 & 3 \\
    Y       & 5 & 4 & 4 & 4 & 4 & 3 \\
    \end{array}
    \]
    Stąd
    \[
    \mathrm{EditDistance}(\text{SNOWY},\text{SUNNY}) = d_{5,5} = 3,
    \]
    czyli potrzebne są trzy podstawienia (np. \(N\to U\), \(O\to N\), \(W\to N\)).
    
    Niech $E(i,j)$ -- edit distance $w_1[1\dots i], w_2[1\dots j]$. Mamy następujące możliwości
    \begin{itemize}
        \item dodanie litery do $w_1 \gets E(i, j-1)+1$
        \item usuniecie litery z $w_2 \gets E(i-1, j)+1$
        \item podmienienie litery w $w_2 \gets E(i-1, j-1)+1$
        \item bez zmian $w_1 \gets E(i-1, j-1)$
    \end{itemize}
    Przy wykonywaniu tego algorytmu musimy brać minimum z tych czterech możliwości, a więc
    \[
        E(i,j) = \min \begin{cases}
            E(i, j-1)+1 \\
            E(i-1, j)+1 \\
            E(i-1, j-1)+1 \\
            E(i-1, j-1)
        \end{cases}
    \]
    A jak wygląda graf? %TODO od szymiego
    \[
        \begin{array}{c|ccccccc}
            d_{i,j} & & 0 & 1 & 2 & 3 & 4 & \dots \\ \hline
                    & 0 & 1 & 2 & 3 & 4 & 5 & \dots \\
            1 & 1 & 0 & 1 & 2 & 3 & 4 & \dots \\
        \end{array}
    \]
    Pseudokod:
    \begin{center}
    \begin{algorithm}
        \caption{Edit Distance}
        \begin{algorithmic}[1]
            \Procedure{EditDistance}{$w_1, w_2$}
                \State $n \gets |w_1|$
                \State $m \gets |w_2|$
                \For{$i=0$ to $n$}
                    \For{$j=0$ to $m$}
                        \If{$i=0$}
                            \State $d(i,j) = j$
                        \ElsIf{$j=0$}
                            \State $d(i,j) = i$
                        \Else
                            \State $d(i,j) = \min\begin{cases}
                                d(i-1,j)+1\\
                                d(i,j-1)+1\\
                                d(i-1,j-1)+1\\
                                d(i-1,j-1)
                            \end{cases}$
                        \EndIf
                    \EndFor
                \EndFor
                \State \textbf{return} $d(n,m)$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \end{center}
    \newpage
    \textbf{Kopiec binarny} \par
    \begin{itemize}
        \item Pełne drzewo binarne przetrzymywane w tablicy
    \end{itemize}
    \vspace{2\baselineskip}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{/home/wojteq18/Pobrane/zdjecia/kp.jpg}
        \label{fig:example_image}
    \end{figure}
    \textbf{Własność kopca (maksymalnego)} \par
    \begin{itemize}
        \item $\forall i$ A[parent[i]] > A[i]
        \item Wysokość węzła = długość najdłuższej prostej ścieżki od tego węzła do liścia 
    \end{itemize}
    \vspace{1\baselineskip}
    \begin{algorithm}[H]
    \caption{Heapify}\label{alg:heapify}
    \begin{algorithmic}[1]
    \Procedure{Heapify}{a}
        \State $largest \gets a$
        \If{$2a \leq size$ \textbf{and} $H[2a] > H[largest]$}
            \State $largest \gets 2a$
        \EndIf
        \If{$2a + 1 \leq size$ \textbf{and} $H[2a + 1] > H[largest]$}
            \State $largest \gets 2a + 1$
        \EndIf
        \If{$largest \neq a$}
            \State Zamień $H[largest]$ i $H[a]$
            \State \Call{Heapify}{$largest$}
        \EndIf
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
    \begin{center}
            Złożoność obliczeniowa procedury Heapify wynosi: $O(log n)$ \par
    \end{center}
    \newpage
    \textbf{Build Heap} \par

    \begin{algorithm} [H]
    \begin{algorithmic}[1] 
    \Procedure{Build-Heap}{} 
      \For{$i \gets \text{size} \text{ down to } 1$} 
        \State \Call{Heapify}{i} 
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
\vspace{1\baselineskip}
Fakt: W n- elementowym kopcu binarnym mamy co najwyżej \par
$\lceil \frac{n}{2^{h-1}} \rceil$ węzłów o wysokości h \par
\vspace{2\baselineskip}

\textbf{Kolejka priorytetowa} \par
\begin{itemize}
    \item Insert (Q, x)
    \item Maximum (Q)
    \item ExtractMax (Q) - zwraca element o największym priorytecie i usuwa go z Q
    \item Increase / Decrease Key (Q, x ,y)
    \item Delete (Q, x)
    \item Union (Q1, Q2) - łączy dwie kolejki priorytetowe w jedną
\end{itemize}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Procedure} & \textbf{Binary Heap} & \textbf{Binomial Heap} & \textbf{Fibonacci Heap} \\
\midrule
Insert        & $O(\log n)$ & $O(\log n)$ & $O(1)$      \\
Maximum       & $O(1)$      & $O(\log n)$ & $O(1)$      \\
ExtractMax    & $O(\log n)$ & $O(\log n)$ & $O(\log n)$ \\
IncreaseKey   & $O(\log n)$ & $O(\log n)$ & $O(1)$      \\
DecreaseKey   & $O(\log n)$ & $O(\log n)$ & $O(1)$      \\
Delete        & $O(\log n)$ & $O(\log n)$ & $O(\log n)$ \\
Union         & $O(n)$      & $O(\log n)$ & $O(1)$      \\
\bottomrule
\end{tabular}
\end{table}
\begin{center}
    Złożoności obliczeniowe w przypadku kopca Fibonacciego zostały podane \par
    jako wartości zamortyzowane - koszt operacji jest średnią z wielu kosztów, \par
    pojedyncze wartości mogą być czasem wolniejsze
\end{center}
\newpage
\begin{center}
    \textbf {Graf} \par
    \vspace{1\baselineskip}
    G = (V, E) \par
    V $\rightarrow$ zbiór wierzchołków {1..n} \par
    E $\in \{\{i, j\}: i,j \in V, i \neq j\}$ \par
\end{center} \par
W grafie nieskierowanym nie rozróżniamy i i j, traktujemy to jako zbiór. \par
W grafie skierowanym i i j traktujemy jako parę, więc kolejność jest istotna. \par
W oznaczeniach będziemy używać: $|V| = n \text{ i } |E| = m$ \par
\vspace{1\baselineskip}
     \begin{figure}[H]
        \centering
        \includegraphics[width=0.3\textwidth]{/home/wojteq18/Pobrane/zdjecia/ls.png}
        \label{fig:example_image}
    \end{figure} \par
\begin{center}
    Powyższa grafika to lista sąsiedztwa, jest to reprezentacja grafu, \par
    umożliwiająca jego obróbkę z użyciem programów komputerowych. \par
\end{center}
\vspace{1\baselineskip}
\textbf{Złożoność pamięciowa: } O(n  + m) = O(|V| + |E|) - wielkość grafu \par
\vspace{1\baselineskip}
\newpage
Następnym sposobem reprezentacji grafu jest macierz sąsiedztwa: \par
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Obrazy/Screenshots/Screenshot From 2025-05-20 12-01-56.png}
    \label{fig:example_image}
\end{figure} \par
\vspace{1\baselineskip}

\begin{algorithm}[H]
\caption{Explore}
\begin{algorithmic}[1]
\Procedure{Explore}{G, v}
    \State $visited(v) \gets$ true
    \State \Call{Previsit}{v}
    \For{each $(v, u) \in E$}
        \If{not $visited(u)$}
            \State \Call{Explore}{G, u}
        \EndIf
    \EndFor
    \State \Call{Postvisit}{v}
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{center}
    Powyższa procedura przeszukuje graf w głąb, dzięki niej \par
    Możemy dotrzec do wszystkich wierzchołków z danego wierzchołka \par
\end{center}
\vspace{1\baselineskip}
\begin{algorithm}[H]
\caption{DFS (Depth First Search)}
\begin{algorithmic}[1]
\Procedure{DFS}{G}
    \For{each $v \in V$}
        \State $visited(v) \gets$ false
    \EndFor
    \For{each $v \in V$}
        \If{not $visited(v)$}
            \State \Call{Explore}{G, v}
        \EndIf
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{center}
    Przeszukanie grafu w głąb (DFS) \par
\end{center}

\end{document} 