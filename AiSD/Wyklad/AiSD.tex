\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[polish]{babel}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{float}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsmath}




\title{Algorytmy i Struktury Danych}
\author{Wojciech Typer}
\date{}

\begin{document}
\maketitle

\begin{algorithm}[H]
\caption{Insertion Sort}\label{alg:insertion_sort}
\begin{algorithmic}[1]
\Procedure{InsertionSort}{A, n}
    \For{$i = 1$ to $n-1$}
        \State $key = A[i]$
        \State $j = i - 1$
        \While{$j \geq 0$ and $A[j] > key$}
            \State $A[j+1] = A[j]$
            \State $j = j - 1$
        \EndWhile
        \State $A[j+1] = key$
    \EndFor
\EndProcedure
\end{algorithmic}
\end{algorithm} 
\vspace{1\baselineskip}
\textbf{Złożoność czasowa:} $O(n^2)$ \par
\textbf{Best case:} w najlepszym przypadku złożoność czasowa będzie wynosić $O(n)$ \par
\textbf{Złożoność pamięciowa:} $O(1)$
\vspace{2\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{/home/wojteq18/Pobrane/zdjecia/insert-sort.png}
    \label{fig:example_image}
\end{figure}
\vspace{3\baselineskip}

\begin{algorithm}[H]
    \caption{Merge Sort}\label{alg:merge_sort}
    \begin{algorithmic}[1]
    \Procedure{MergeSort}{A, 1, n}
        \If{|A[1..n]| == 1} 
            \State \Return{A[1..n]}
        \Else
            \State $B = \text{MergeSort}(A, 1, \lfloor n/2 \rfloor)$
            \State $C = \text{MergeSort}(A, \lfloor n/2 \rfloor, n)$
            \State \Return{Merge(B, C)}
        \EndIf
    \EndProcedure
    \end{algorithmic}    
\end{algorithm}
\begin{algorithm}[H]
    \caption{Merge}\label{alg:merge}
    \begin{algorithmic}[1]
    \Procedure{Merge}{X[1..k], Y[1..n]}
        \If{$X = \emptyset$}
            \State \Return{$Y$}
        \ElsIf{$Y = \emptyset$}
            \State \Return{$X$}
        \ElsIf{$X[1] \leq Y[1]$}
            \State \Return{$[X[1]] \times \text{Merge}(X[2..k], Y[1..n])$}   
        \Else
            \State \Return{$[Y[1]] \times \text{Merge}(X[1..k], Y[2..n])$}
        \EndIf
    \EndProcedure
    \end{algorithmic}       
\end{algorithm}
\vspace{1\baselineskip}
\textbf{Złożoność czesowa Merge Sort:} $O(n \log n)$ \par
\textbf{Złożoność pamięciowa Merge Sort:} $O(n)$
\vspace{2\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/Merge_sort_algorithm_diagram.svg.png}
    \label{fig:example_image}
\end{figure}
\vspace{2\baselineskip} \par
Istnieje również iteracyjna wersja algorytmu Merge, sort, która została \par
przedstawiona poniżej w postaci pseudokodu.
\begin{algorithm}[H]
    \caption{IterativeMergeSort}\label{alg:iterative_merge}
    \begin{algorithmic}[1]
        \Procedure{IterativeMergeSort}{A[1..n]}
            \For{$size = 1$ \textbf{to} $n-1$ \textbf{by} $size \times 2$}
                \For{$left = 0$ \textbf{to} $n-1$ \textbf{by} $2 \times size$}
                    \State $mid \gets \min(left + size - 1, n - 1)$
                    \State $right \gets \min(left + 2 \times size - 1, n - 1)$
                    \State \Call{Merge}{A, left, mid, right}
                \EndFor
            \EndFor
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\vspace{1\baselineskip}
\textbf{Złożoność czasowa Iterative Merge Sort:} $O(n \log n)$ - dzieje się tak, \par
ponieważ size jest podwajany o 2 w każdej iteracji, więc potrzebujemy \par
około $ \log_2 n$ iteracji, a w każdej z nich wykonujemy $O(n)$ operacji. \par
\vspace{1\baselineskip}
\textbf{Złożoność pamięciowa Iterative Merge Sort:} $O(n)$ \par
\vspace{1\baselineskip}
\textbf{Notacja asymptotyczna}
O:f(n) = O(g(n)) $\rightarrow (\exists c > 0) (\exists n_0 \in N) : \forall n \geq n_0 \rightarrow 0 \leq f(n) \leq c \cdot g(n)$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/images.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
$f(n) = O(g(n)) \rightarrow lim_{n \to \infty} \frac{|f(n)|}{|g(n)|} < \infty$ \par
\vspace{1\baselineskip}
\textbf{Notacja asymptotyczna - własności} \par
\vspace{1\baselineskip}
a) $f(n) = n^3 + O(n^2) \rightarrow (\exists h(n) = O(n^2))(f(n) = n^3 + h(n))$ \par
\vspace{1\baselineskip}
b) $n^2 + O(n) = O(n^2) \rightarrow (\forall f(n) = O(n))(\exists h(n) = O(n^2))(n^2 + f(n)-h(n))$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\Omega$} \par
\vspace{1\baselineskip}
$f(n) = \Omega (g(n)) \rightarrow(\exists c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(c * g(n) \leq |f(n))$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/1a7d9c3f882e7a237b30f5eb6defa1aa45c6ab22.png}
    \label{fig:example_image}
\end{figure}
\vspace{7\baselineskip}
\textbf{Notacja $\Omega$ - własności} \par
\vspace{1\baselineskip}
a) $n^3 = \Omega (2n^2)$ \par
\vspace{1\baselineskip}
b) $n = \Omega (log(n))$ \par
\vspace{1\baselineskip}
c) $2n^2 = \Omega (n^2)$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\Theta$} \par
\vspace{1\baselineskip}
$f(n) = \Theta (g(n)) \rightarrow (\exists c_1, c_2 > 0)(\exists n_0 \in N)(\forall n  \geq n_0)(c_1 g(n) \leq $ \par $|f(n) \leq c_2 g(n))$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/theeta.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
$\Theta (f(n)) = O(f(n)) \cap \Omega (f(n))$ \par
\vspace{1\baselineskip}
\textbf{Notacja o- małe} \par
\vspace{1\baselineskip}
$f(n) = o(g(n)) \rightarrow (\forall c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(|f(n)| < c * |g(n)|)$ \par
\vspace{1\baselineskip}
\textbf{Notacja o- małe - przykłady} \par
\vspace{1\baselineskip}
a) $117n log(n) = o(n^2)$ \par
\vspace{1\baselineskip}
b) $ n^2 = o(n^3)$ \par
\vspace{1\baselineskip}
\textbf{Notacja $\omega$} \par
\vspace{1\baselineskip}
$f(n) = \omega (g(n)) \rightarrow (\forall c > 0)(\exists n_0 \in N)(\forall n \geq n_0)(|f(n)| > c * |g(n)|)$ \par
\vspace{1\baselineskip}
$lim_{n \to \infty} \frac{|f(n)|}{|g(n)|} = \infty$ \par
\vspace{1\baselineskip}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{/home/wojteq18/Pobrane/zdjecia/allnotations.png}
    \label{fig:example_image}
\end{figure}
\vspace{1\baselineskip}
\textbf{Rekurencje} \par
\vspace{1\baselineskip}
Metoda podstawiania (metoda dowodzenia indukcyjnego) \par
    \hspace{20pt}1. Zgadnij odpowiedź (bez stałych) \par
    \hspace{20pt}2. Sprawdź przez indukcję, czy dobrze zgadliśmy \par
    \hspace{20pt}3. Znajdź stałe \par
\vspace{1\baselineskip}
Przykład 1: \par
\vspace{1\baselineskip}
$T(n) = 4T(\frac{n}{2}) + n$   \par
Pierwszy strzał: $T(n) = O(n^3)$ \par
Cel: pokazać, że $(\exists c > 0) T(n) \leq c * n^3$ \par
Krok początkowy: $T(1) = \Theta (1) = c * 1^3 = c$ \par
Krok indukcyjny: zał. że, $(\forall_(k < n)) (T(k) \leq c * k^3) = $ \par
Dowód: $T(n) = 4T(\frac{n}{2}) + n \leq 4c * (\frac{n}{2})^3 + n = \frac{1}{2}cn^3 + n =$ \par
$= cn^3 - \frac{1}{2}cn^3 + n = cn^3 - (\frac{1}{2}cn^3 - n) \leq cn^3$ \par
Pokazaliśmy, że $T(n) = O(n^2)$ \par
\vspace{1\baselineskip}
Spróbujmy wzmocnić zał. indukcyjne: $T(n) \leq c_1 n^2 - c_2 n$ \par
$T(n) \leq 4T(\frac{n}{2}) + n \leq 4(c_1 (\frac{n}{2})^2 - c_2 \frac{n}{2}) + n = $ \par
$ = c_1 n^2 - 2c_2 n + n = c_1 n^2 - (2c_2 - 1)n \leq c_1 n^2 - c_2 n$ \par
Musimy dobrać takie $c_1 i c _2$, aby $2c_1 \geq c_2$ \par
Wówczas otrzymamy $T(1) = O(1) \leq c_1 1^2 - c_2 1$ \par
\vspace{9\baselineskip}
Przykład 2: \par
\vspace{1\baselineskip}
$T(n) = 2T(\sqrt{n}) + log(n)$ \par
Załóżmy, że n jest potęgą dwójki $n = 2^m \rightarrow m = log(n)$ \par
$T(2^m) = 2T(2^{m/2}) + m$ \par
oznaczmy $T(2^m) = S(m)$ \par
$T(2^m) = 2T(2^{m/2}) + m \rightarrow 2S(m/2) + m$ \par
$S(m) = O(m log(m))$ \par
$T(n) = O(log(n) log(log(n)))$ (formalnie powinniśmy to udowodnić) \par  
\vspace{1\baselineskip}
\textbf{Drzewo rekursji} \par
\vspace{1\baselineskip}
Przykład : $T(n) = T(\frac{n}{2}) +T (\frac{n}{4}) + n^2$ \par
\vspace{1\baselineskip}
\begin{center}
    \begin{tikzpicture}
        [level distance=1.5cm,
        level 1/.style={sibling distance=4cm},
        level 2/.style={sibling distance=2cm}]
        \node {$n^2$}
            child {node {$\frac{n^2}{4}$}
                child {node {$\frac{n^2}{16}$}}
                child {node {$\frac{n^2}{64}$}}
            }
            child {node {$\frac{n^2}{16}$}
                child {node {$\frac{n^2}{64}$}}
                child {node {$\frac{n^2}{256}$}}
            };
          \node[draw=none] at (-4.5,0) {$n^2$};
          \node[draw=none] at (-4.5,-1.5) {$\frac{5}{16}n^2$};
          \node[draw=none] at (-4.5,-3) {$\frac{25}{256}n^2$};
    \end{tikzpicture}
\end{center}
\vspace{1\baselineskip}
Trzeba pamiętać, że drzewo rekursji samo w sobie nie jest formalnym rozwiązaniem problemu. Nie można go urzywać do dowodzenia złożoności algorytmów.
Jest to jedynie intuicyjne podejście do problemu. Formmalnie T(n) należałoby policzyć jako sumę wszystkich wierzchołków w drzewie rekursji:
\[
    T(n) = \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k \cdot n^2 = n^2 \sum^{\infty}_{k=0} \left(\frac{5}{16}\right)^k = n^2 \frac{1}{1-\frac{5}{16}} = n^2 \frac{16}{11} = \frac{16}{11}n^2
\]
Widzimy zatem, że $T(n) = O(n^2)$ \par
\vspace{1\baselineskip}
\textbf{Master Theorem} \par
\vspace{1\baselineskip}
Niech $a \geq 1, b > 1, f(n), d \in N$ oraz $f(n)$ będzie funkcją nieujemną. Rozważmy rekurencję: \par
\[
    T(n) = aT(\frac{a}{b}) + \Theta(n^d)
\] 
Wówczas: \par
\begin{itemize}
    \item $\Theta(n^d)$, jeśli $d > log_b a$
    \item $\Theta (n^d log(n))$, jeśli $d = log_b a$
    \item $\Theta(n^{log_b a})$, jeśli $d < log_b a$
\end{itemize}
Do przedstawienia problemu użyjemy drzewa rekursji. Rozważmy rekurencję:
\[
    T(n) = aT(\frac{n}{b}) + \Theta(n^d)
\]
\begin{center}
\begin{tikzpicture}
    [level distance=1.5cm,
    level 1/.style={sibling distance=4cm},
    level 2/.style={sibling distance=2cm}]
    \node {$c \cdot n^d$}
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        }
        child {node {$c \cdot \left(\frac{n}{b}\right)^d$}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
            child {node {$c \cdot \left(\frac{n}{b^2}\right)^d$}}
        };
      \node[draw=none] at (-4.5,0) {$n^d$};
      \node[draw=none] at (-4.5,-1.5) {$\frac{n^d}{b^d}$};
      \node[draw=none] at (-4.5,-3) {$\frac{n^d}{b^{2d}}$};
  \end{tikzpicture} 
\end{center}
\begin{enumerate}
    \item suma kosztoów w $k$--tym kroku
        \[
            a^k c (\frac{n}{b^k})^d = c (\frac{a}{b^d})^k n^d
        \]
        gdzie $c(\frac{n}{b^k})^d$ to koszt jednego podproblemu w $k$--tym kroku
    \item obliczenie wysokości drzewa:
        \[
            \frac{n}{b^h} = 1 \rightarrow h = \log_b n
        \]
    \item Obliczenie $T(n)$
    \begin{align*}
        T(n) &= \Theta\left(\sum^{\log_b n}_{k=0} c\frac{a}{b^k}n^d\right) \\
             &= \Theta\left(c \cdot n^d \sum^{\log_b n}_{k=0} \left(\frac{a}{b^d}\right)^k\right) \\
             &= \Theta\left(c \cdot n^d \frac{1-\left(\frac{a}{b^d}\right)^{\log_b n + 1}}{1-\frac{a}{b^d}}\right) \\
             &\implies T(n) = \Theta(n^d)
    \end{align*}
    
    \item rozważmy 3 przypadki:
        \begin{enumerate}
            \item $d > \log_b a$ 
                \[
                    T(n) = \Theta(n^d)
                \]
            \item $d = \log_b a$ 
                \[
                    T(n) = \Theta(n^d \log n)
                \]
            \item $d < \log_b a$
                \[
                    T(n) = \Theta(n^{\log_b a})
                \]
        \end{enumerate}
\end{enumerate}

\subsubsection*{Przykłady}
\begin{itemize}
    \item $T(n) = 4T(\frac{n}{2}) + 11n$ \newline
        Wtedy kożystając z \textbf{Master Theorem} mamy:
        \[
            a = 4, b = 2, d = 1
        \]
        Jak i również
        \[
            \log_b a = \log_2 4 = 2 > 1 = d \implies T(n) = \Theta(n^2)
        \]
    \item $T(n) = 4T(\frac{n}{3}) + 3n^2$ \newline
        Wtedy
        \[
            a = 4, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 4 < 2 = d \implies T(n) = \Theta(n^2)
        \]
    \item $T(n) = 27T(\frac{n}{3}) + \frac{n^2}{3}$ \newline
        Wtedy
        \[
            a = 27, b = 3, d = 2
        \]
        Jak i również
        \[
            \log_b a = \log_3 27 = 3 > 2 = d \implies T(n) = \Theta(n^3\log n)
        \]
\end{itemize}

\subsection*{Metoda dziel i zwyciężaj (D\&C)}
Na czym ona polega?
\begin{enumerate}
    \item Podział problemu na mniejsze podproblemy 
    \item Rozwiazanie rekurencyjnie mniejsze podpoblemy
    \item połącz rozwiązania podproblemów w celu rozwiązania problemu wejściowego
\end{enumerate}
\subsubsection*{Algorytm -- Binary Search}
\begin{itemize}
    \item \textbf{Input}: posortowania tablica \texttt{A[1..n]} oraz element \texttt{x}
    \item \textbf{Output}: indeks \texttt{i} taki, że \texttt{A[i] = x} lub \texttt{0} jeśli \texttt{x} nie występuje w \texttt{A}
        \item przebieg algorytmu: 
            \begin{algorithm}[H]
                \caption{Binary Search}
                \begin{algorithmic}[1]
                    \Procedure{BinarySearch}{A, x}
                        \State $l = 1$
                        \State $r = |A|$
                        \While{$l \leq r$}
                            \State $m = \lfloor \frac{l+r}{2} \rfloor$
                            \If{$A[m] = x$}
                                \State \Return{$m$}
                            \ElsIf{$A[m] < x$}
                                \State $l = m + 1$
                            \Else
                                \State $r = m - 1$
                            \EndIf
                        \EndWhile
                        \State \Return{0}
                    \EndProcedure
                \end{algorithmic}
            \end{algorithm}
        \item \textbf{Asypmtotyka}
            Algorytm spełnia następująca rekurencje:
            \[
                T(n) = T(\frac{n}{2}) + \Theta(1)
            \]
            Rozwiązując za pomocą \textbf{Master Theorem} otrzymujemy:
            \[
                T(n) = \Theta(\log n)
            \]
\end{itemize}
\vspace{1\baselineskip}
\subsubsection*{Divide \& Conquer}

\textbf{Problem:} Obliczenie $x^n$.  

Rozwiązanie naiwną metodą iteracyjną:  
\[
x^n = x \cdot x \cdot \dots \cdot x \quad \Rightarrow \quad \Theta(n)
\]

Rozwiązanie za pomocą Divide \& Conquer:  

\[
x^n =
\begin{cases}
    (x^{\frac{n}{2}}) \cdot (x^{\frac{n}{2}}), & \text{gdy } n \text{ jest parzyste} \\
    (x^{\frac{n-1}{2}}) \cdot (x^{\frac{n-1}{2}}) \cdot x, & \text{gdy } n \text{ jest nieparzyste}
\end{cases}
\]

Rekurencyjna złożoność czasowa:
\[
T(n) = T(n/2) + \Theta(1) = \Theta(\log n)
\]

---

\textbf{Problem:} Obliczenie $n$-tej liczby Fibonacciego  

Metoda rekurencyjna:
\[
F(n) = F(n-1) + F(n-2)
\]
Ma ona złożoność wykładniczą:
\[
\Theta (\phi^n), \quad \text{gdzie } \phi = \frac{1 + \sqrt{5}}{2}
\]

Drzewo rekurencyjne dla $F_4$:

\begin{center}
\begin{tikzpicture}[level distance=1.5cm,
    level 1/.style={sibling distance=6cm},
    level 2/.style={sibling distance=3cm},
    level 3/.style={sibling distance=1.5cm}]
    
  \node {$F_4$}
      child {node {$F_3$}
          child {node {$F_2$}
              child {node {$F_1$}}
              child {node {$F_0$}}
          }
          child {node {$F_1$}}
      }
      child {node {$F_2$}
          child {node {$F_1$}}
          child {node {$F_0$}}
      };

\end{tikzpicture}
\end{center}

\textbf{Wzór jawny:}
\[
F_n = \frac{1}{\sqrt{5}} \left( \phi^n - (-\phi)^{-n} \right)
\]


Obliczanie $F_n$ macierzą:
Zamiast rekurencji można użyć potęgowania macierzy, co daje optymalną złożoność.  
Dla każdego $n \geq 0$ zachodzi:

\[
\begin{bmatrix}
    1 & 1 \\
    1 & 0
\end{bmatrix}^n
=
\begin{bmatrix}
    F_{n+1} & F_n \\
    F_n & F_{n-1}
\end{bmatrix}
\]

Potęgowanie macierzy metodą szybkiego potęgowania daje czas:
\[
\Theta(\log n)
\]
co jest znaczną poprawą w porównaniu do wykładniczej rekurencji.

\textbf{Mnożenie liczb binarnych metodą Divide \& Conquer}

\textbf{Wejście:} $x, y$  
\textbf{Wyjście:} $x \cdot y$

Każdą liczbę można rozbić na dwie połowy:
\[
x = x_L \cdot 2^{\frac{n}{2}} + x_R
\]
\[
y = y_L \cdot 2^{\frac{n}{2}} + y_R
\]

Podstawiając do iloczynu:
\[
xy = (x_L \cdot 2^{\frac{n}{2}} + x_R) \cdot (y_L \cdot 2^{\frac{n}{2}} + y_R)
\]

Po rozwinięciu:
\[
xy = x_L y_L \cdot 2^n + (x_L y_R + x_R y_L) \cdot 2^{\frac{n}{2}} + x_R y_R
\]

Rekurencyjna zależność czasowa:
\[
T(n) = 4T(n/2) + \Theta(n)
\]

Zastosowanie \textbf{Master Theorem} daje:
\[
T(n) = \Theta(n^2)
\]
co pokazuje, że metoda ta nie poprawia złożoności względem standardowego mnożenia. 

\vspace{1\baselineskip}
\textbf{Optymalizacja: metoda Gaussa}

Zamiast wykonywać 4 mnożenia rekursywne, można zastosować zasadę Gaussa:
\[
xy = x_L y_L \cdot 2^n + ((x_L + x_R)(y_L + y_R) - x_L y_L - x_R y_R) \cdot 2^{\frac{n}{2}} + x_R y_R
\]

Dzięki temu zamiast 4 mnożeń wykonujemy tylko 3:
\[
T(n) = 3T(\frac{n}{2}) + \Theta(n)
\]

Zastosowanie \textbf{Master Theorem} daje:
\[
T(n) = \Theta(n^{\log_2 3})
\]

\begin{algorithm}[H]
    \caption{Multiply - Mnożenie dużych liczb binarnych metodą Gaussa}
    \label{alg:multiply}
    \begin{algorithmic}[1]
        \Procedure{multiply}{x, y}
            \State $n \gets \max(|x|, |y|)$ 
            \If{$n = 1$} 
                \State \Return{$x \cdot y$}
            \EndIf
            \State $m \gets \lceil {n/2} \rceil$
            \State $x_L, x_R \gets$ 
            \State $y_L, y_R \gets$ 
            \State $p_1 \gets \Call{multiply}{x_L, y_L}$
            \State $p_2 \gets \Call{multiply}{x_R, y_R}$
            \State $p_3 \gets \Call{multiply}{(x_L + x_R), (y_L + y_R)}$
            \State \Return{$p_1 \cdot 2^{2m} + (p_3 - p_1 - p_2) \cdot 2^m + p_2$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\textbf{QuickSort}
\begin{algorithm}[H]
    \caption{QuickSort - Sortowanie szybkie}
    \label{alg:quicksort}
    \begin{algorithmic}[1]
        \Procedure{quicksort}{A, low, high}
            \If{$low < high$}
                \State $p \gets \Call{partition}{A, low, high}$
                \State \Call{quicksort}{A, low, p - 1}
                \State \Call{quicksort}{A, p + 1, high}
            \EndIf
        \EndProcedure
        \\
        \Procedure{partition}{A, low, high}
            \State $pivot \gets A[high]$
            \State $i \gets low - 1$
            \For{$j \gets low$ \textbf{to} $high - 1$}
                \If{$A[j] \leq pivot$}
                    \State $i \gets i + 1$
                    \State \Call{swap}{A[i], A[j]}
                \EndIf
            \EndFor
            \State \Call{swap}{A[i + 1], A[high]}
            \State \Return{$i + 1$}
        \EndProcedure
    \end{algorithmic}
\end{algorithm}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/quick.png}
    \label{fig:example_image}
\end{figure}

\begin{algorithm}[H]
    \caption{Hoare Partition}
    \label{alg:quicksort}
    \begin{algorithmic}[1]
        \Procedure{hoare\_partition}{A, p, q}
            \State $pivot \gets A\left[\left\lfloor \frac{p + q}{2} \right\rfloor\right]$
            \State $i \gets p - 1$
            \State $j \gets q + 1$
            \While{true}
                \Repeat
                    \State $i \gets i + 1$
                \Until{$A[i] \geq pivot$}
                \Repeat
                    \State $j \gets j - 1$
                \Until{$A[j] \leq pivot$}
                \If{$i \geq j$}
                    \State \Return $j$
                \EndIf
                \State swap($A[i], A[j]$)
            \EndWhile
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\par\textbf{Analiza worst-case QuickSorta}

\par
$T(n) = T(n - 1) + T(0) + \Theta(n) = T(n - 1) + \Theta(n)$

\par
Drzewo rekurencji (dla przypadku pesymistycznego, tj. jednostronny podział):

\begin{center}
\begin{tikzpicture}[
    level distance=1.4cm,
    every node/.style={circle,draw},
    level 1/.style={sibling distance=5cm},
    level 2/.style={sibling distance=3cm},
    level 3/.style={sibling distance=2cm},
    level 4/.style={sibling distance=1.5cm}
    ]
\node {$c_n$}
  child {node {$c_{n-1}$}
    child {node {$c_{n-2}$}
      child {node {$c_{n-3}$}
        child {node {$\cdots$}
          child {node {$c_1$}
            child[missing]
            child[missing]
          }
          child[missing]
        }
        child[missing]
      }
      child[missing]
    }
    child[missing]
  }
  child {node {$\Theta(1)$}}; % prawa strona (najmniejsza partycja)
\end{tikzpicture}
\end{center}


\par
$T(n) \leq \sum_{i=1}^{n} c \cdot i = c \cdot \sum_{i=1}^{n} i = \Theta(n^2)$
\vspace{1\baselineskip}

\par\textbf{Analiza best-case}

\par
Jeśli pivot zawsze dzieli tablicę na dwie równe części:

\[
T(n) = 2T\left(\frac{n}{2}\right) + \Theta(n)
\Rightarrow T(n) = \Theta(n \log n)
\]

\par\textbf{Analiza average-case}

\par
Niech $T_n$ oznacza liczbę porównań dla tablicy długości $n$.

\[
x_k =
\begin{cases}
    1, & \text{jeśli partition dzieli tablicę na } (k,\; n - k - 1) \\[0.5em]
    0, & \text{w przeciwnym wypadku}
\end{cases}
\]


\[
T_n = \sum_{k=0}^{n-1} x_k \cdot (T_k + T_{n-k-1}) + (n - 1)
\]

\par
Liczymy wartość oczekiwaną:

\[
E(T_n) = \sum_{k=0}^{n-1} \mathbb{E}(x_k) \cdot \left( \mathbb{E}(T_k) + \mathbb{E}(T_{n-k-1}) \right) + (n - 1)
\]

\[
\mathbb{E}(x_k) = \frac{1}{n} \quad \text{(bo pivot jest losowy)}
\]

\[
E(T_n) = \frac{1}{n} \sum_{k=0}^{n-1} \left( E(T_k) + E(T_{n-k-1}) \right) + (n - 1)
\]

\[
= \frac{2}{n} \sum_{k=0}^{n-1} E(T_k) + (n - 1)
\]

\[
\Rightarrow E(T_n) = \Theta(n \log n)
\]

    \vspace{1\baselineskip}
    \textbf{Analiza avg Case'a}
    $T_n \rightarrow$ Liczba porównań elementów sortowanej tablicy: |A| = n \newline

\[
x_k =
\begin{cases}
    1, & \mbox{jeśli partition dzieli tablicę na } (k,\, n - k - 1) \\[0.6em]
    0, & \mbox{w przeciwnym wypadku}
\end{cases}
\]


\[
    T_n =
    \begin{cases}
        T_0 + T_{n-1} + n-1, gdy (0, n-1) -split \\
        T_1 + T_{n-2} + n -1, gdy (1, n-2) -split \\
        ... \\
        T_k + T_{n-1-k} + n - 1, gdy (k, n-k-1) -split \\
        ... \\
        T_{n-1} + T_0 + n - 1, gdy (n-1, o) -split
    \end{cases}
    \]


    $T_n = \sum_{k=0}^{n-1} x_k (T_k + T_{n-k-1}) + n - 1$ \newline
    
    liczymy wartosć oczekiwaną: \newline

    $E(T_n) = E (\sum{k=0}^{n-1} X_k (T_k + T_{n-k-1} + n - 1))$ \newline
    $E(T_n) = \sum_{k=0}^{n-1} E(X_k \cdot (T_k + T_{n-k-1}) + n - 1) $ \newline
    $E(T_n) = \sum_{k=0}^{n-1} E(X_k) - E(T_k + T_{n-k-1} - n - 1)$ \newline
    $E(T_n) = \frac{1}{n} \cdot \sum_{k=0}^{n-1} E(T_k) + \sum (E(T_{n-k-1}))$ \newline


    \textbf{Dual pivot quicksort} \newline

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/dpq.png}
        \label{fig:example_image}
    \end{figure}

    \textbf{Wartość oczekiwana: } 

    $E(\mbox{liczba porównań w dual pivot partition}) \approx \frac{16}{9}n$ 

    $E(\mbox{liczba porównań w dual pivot qs sedwick}) \approx \frac{32}{15}n logn$  \newline


    \textbf{Yaroslavsky dual pivot qs} 

    $E(\mbox{liczba porównań w partition}) \approx \frac{19}{12}n$ 

    $E(\mbox{liczba porównań w Dual Pivot qs Yaroslavsky}) \approx 1.9 n logn$ \newline

    \textbf{Strategia count}

    $E(\mbox{liczba porównań w Count Partition}) \approx \frac{3}{2}n$

    $E(\mbox{liczba porównań w Dual Pivot qs z count}) \approx 1.8 n logn$ \newline

    \textbf{Comparsion Model} 

    Dolne ograniczenie na liczbę porównań w problemie sortowania \par
    w Comparsion Modelwynosi $\Omega(n logn)$ \newline

    D-d: \par
    \begin{itemize}
        \item dla dowolnego algorytmu sortującego możemy znależć odpowiadające mu drzewo decyzyjne
        \item n! liści w binarnym drzewie decyzyjnym
        \item drzewo binarne pełne o wysokości h ma co najmniej $2^h$ liści
        \item ale liści w drzewie decyzyjnym powinno być co najmniej n!, zatem: \par
        $2^h \leq n!$ / lg \par
        $h \leq \log_{2}n!$ \par
        $lg n! = lg(\sqrt{s \pi n} (\frac{n}{e})^n (1 + o(1)))$ \par
        $lg (\frac{n}{e})^n + lg (\sqrt(2 \pi n)(1 + o(1)))$ \par
        $n logn - n lg e + lg(\sqrt{2 \pi n} (2 + o(1))) = \Omega(n logn)$
    \end{itemize}    \par

    Sortowanie: \par
    Input: $|a| = n, \forall i \in \{1, ..., k\}$ \par
    Output: posortowana rosnąco tablica A \par

    \begin{algorithm}[H]
        \caption{CountingSort}
        \label{alg:countingsort}
        \begin{algorithmic}[1]
            \Procedure{counting\_sort}{A, n, k}
                \For{$i = 1$ \textbf{to} $k$}
                    \State $C[i] \gets 0$
                \EndFor
                \For{$i = 1$ \textbf{to} $n$}
                    \State $C[A[i]] \gets C[A[i]] + 1$
                \EndFor
                \For{$i = 2$ \textbf{to} $k$}
                    \State $C[i] \gets C[i] + C[i - 1]$
                \EndFor
                \For{$i = n$ \textbf{downto} $1$}
                    \State $B[C[A[i]]] \gets A[i]$
                    \State $C[A[i]] \gets C[A[i]] - 1$
                \EndFor
                \State \Return $B$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm} \par

    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/cs.png}
        \label{fig:example_image}
    \end{figure} \par

    Złożoność obliczeniowa Counting Sorta: \par
    $\Theta (n + k)$ gdzie $k = O(n)$ \par

    \vspace{1\baselineskip}

    \textbf{Stable Sorting Property} \par

    Algorytm zachowuje kolejność równych sobie elementów z tablicy wejściowej

    \vspace{5\baselineskip}

    \textbf{RadixSort} \par

    \begin{algorithm}[H]
        \caption{RadixSort}
        \label{alg:radixsort}
        \begin{algorithmic}[1]
            \Procedure{radix\_sort}{A, n, d}
                \For{$i = 1$ \textbf{to} $d$}
                    \State $counting\_sort(A, n, 9)$
                \EndFor
                \State \Return $A$
            \EndProcedure
        \end{algorithmic}
    \end{algorithm} \par
    \vspace{1\baselineskip}

    \textbf{Złożoność obliczeniowa RadixSorta} \par
    \begin{itemize}
        \item n liczb b'bitowych
        \item liczb b bitowych dzielimy na r-bitowe cyfry
        \item cyfry są z |{$0,...,2^n-1$}| = $2^n$ \par
        \item Counting Sort sortujący n liczb względem jednej cyfry
    \end{itemize} \par
    Zatem RadixSort będzie miał złożoność obliczneiową: \par
    $\Theta(\frac{b}{r} \cdot (n + 2^r))$ \par
    Co po wykonaniu skomplikowanej analizy daje: \par
    $\Theta(d \cdot n)$ \par

    \vspace{2\baselineskip}
    \textbf{Statystyki pozycyjne} \par
    \vspace{1\baselineskip} \par
    \textbf{Def: } k-tą statystyką pozycyjną nazywam← k-tą najmniejszą wartość z \par
    zadanego zbioru \par
    przykład: \par
    \begin{itemize}
        \item $k=1 \rightarrow O(n)$
        \item $k=n$ $\rightarrow O(n)$
        \item $k=\lfloor \rightarrow \text { sortowanie } O(n \log n)$
    \end{itemize}
    \vspace{1\baselineskip}
    \begin{algorithm}[H]
        \caption{RandomSelect}
        \label{alg:randomselect}
        \begin{algorithmic}[1]
            \Procedure{random\_select}{A, p, q, i}
                \If{$p = q$}
                    \State \Return $A[p]$
                \EndIf
                \State $r \gets$ RandPartition($A$, $p$, $q$)
                \State $k \gets r - p + 1$
                \If{$i = k$}
                    \State \Return $A[r]$
                \ElsIf{$i < k$}   
                    \State \Return \Call{random\_select}{$A$, $p$, $r - 1$, $i$}
                \Else
                    \State \Return \Call{random\_select}{$A$, $r + 1$, $q$, $i - k$}
                \EndIf         
            \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    
    \vspace{1\baselineskip}
    \textbf{Select algorithm} \par
    \begin{itemize}
        \item dzielimy A[p..q] na $\frac{n}{\lfloor 5 \rfloor}$ pięcioelementowych częsci
        oraz ostanią część na $\leq$ 5 elementów
        \item Sortujemy te grupy i wybieramy z każdej z nich medianę
        \item Znajdujemy medianę M. Select(M, 1, $\frac{n}{5}$, $\frac{n}{10}$)
        \item Ustalamy X jako pivot; Partition(A, p, q) i tak samo jak w RandomSelect
    \end{itemize}

    \textbf{Select} \par
    Select(A, K) $\rightarrow$ T(n) \par
    \begin{itemize}
        \item Dziel na 5 elementowe tablice i znajdź ich medianę $\rightarrow \Theta(n)$
        \item Select (...) $\rightarrow$ znajdź medianę median $\rightarrow T(\lceil{\frac{n}{5}}\lceil)$ \par
        \item Użyj mediany median jako pivot w Partition $\rightarrow \Theta(n)$
        \item Idź do lewej albo prawej podtablicy w zależności od indeksu pivota i szukaj statystyki pozycyjnej
    \end{itemize}
    Otrzymujemy: $t(n) = T(\lceil{\frac{n}{5}}\lceil) + \Theta(?)$ \par
    \vspace{9\baselineskip}
    \textbf{Struktury danych} \par
    Set interface: \par
    \begin{itemize}
        \item build (A) - buduje set z danych zawartych w A
        \item length - zwraca moc zbioru
        \item find (k) - zwraca element zbioru o kluczu równym k
        \item insert (k) - dodaje element o kluczu k do zbioru
        \item delete (k) - usuwa element o kluczu k ze zbioru
        \item find\_max - zwróc element o największym kluczu
        \item find\_min - zwróć element o najmniejszym kluczu
        \item find\_prev - zwraca element poprzedni od klucza
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Binary Search Tree} \par
    BST property : \par
    \begin{itemize}
        \item x $\in$ T - x jest węzłem drzewa T
        \item Wówczas każdy y $in$ x.left ma y.key < x.key
        \item key y $in$ x.right ma y.key > x.key
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Inorder Tree Walk} \par
    \begin{algorithm}[H]
        \caption{Inorder Tree Walk}\label{alg:inorder_tree_walk}
        \begin{algorithmic}[1]
        \Procedure{InorderTreeWalk}{x $\in$ T}
            \If{$x \neq \text{null}$}
                \State \Call{InorderTreeWalk}{x.left}
                \State print(x)
                \State \Call{InorderTreeWalk}{x.right}
            \EndIf
        \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \newpage
    \textbf{Tree Search} \par
    \begin{algorithm}[H]
        \caption{TreeSearch}\label{alg:tree_search}
        \begin{algorithmic}[1]
        \Procedure{TreeSearch}{x $\in$ T, k}
            \If{$x = \text{null} \lor k = x.key$}
                \State \Return x
            \ElsIf{$k < x.key$}
                \State \Return \Call{TreeSearch}{x.left, k}
            \Else
                \State \Return \Call{TreeSearch}{x.right, k}
            \EndIf
        \EndProcedure
        \end{algorithmic}
    \end{algorithm}
    \vspace{1\baselineskip}
    \textbf{BST - Delete} \par
    \begin{itemize}
        \item x jest liścciem - zwolnij pamięć zajmowaną przez x, wstaw wskaźnik na jego ojca (na niego / na null'a)
        \item x ma jedno poddrzewo - x ma syna v to: \par
        \begin {itemize}
            \item zwalniamy pamięć x
            \item ojciec x wskazuje na v
            \item v.p wskazuje na x.p
        \end{itemize}
        \item x ma dwa poddrzewa: \par
        \begin{itemize}
            \item znajdź następnika x -> y
            \item zastąp dane x danymi z y
            \item skasuj y
        \end{itemize}
    \end{itemize}
    \vspace{1\baselineskip}
    \textbf{Twierdzenie: } Niech T będzie losowym drzewem  BST o n-węzłach. wtedy: \par
    
    $E(h(t)) \leq 3log_2{n} = o(logn)$ \par

    D-d: \par
    Nierówność Jensena: f-wypukła: \par
    $f(E(x)) \leq E(f(x))$

    Zamiast analizować zmienną losową h(t) będziemy się zajmować zmienną losową $H_{n}$, będziemy się zajmować $Y_{n} = 2^{H_{n}}$ \par

    Pokażemy, że $E(Y_{n}) = O(n^3)$ \par

    $2^{H_{n}} \leq E(2^{H_{n}}) = E(Y_{n}) = O(n^3) //log_{2}$ \par
    $E(H_{n}) = 3 \cdot log_2{n} + o(lnn)$ \par
    \newpage
    \textbf{Drzewa czerwono-czarne} \par   
    \begin{itemize}
        \item Drzewo czerwono-czarne jest drzewem BST
        \item Każdy węzeł jest czerwony albo czarny
        \item Korzeń oraz liście są czarne
        \item Czerwony węzeł nie może mieć czerwonego ojca
        \item Każda ścieżka od węzła do liścia ma tę samą liczbę czarnych węzłów (ścieżkę tę będziemy nazywać black-height i oznaczać jako bh(x))
    \end{itemize}
    \vspace{1\baselineskip}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.7\textwidth]{/home/wojteq18/Pobrane/zdjecia/rbt.png}
        \label{fig:example_image}
    \end{figure} \par
    \vspace{1\baselineskip}
    \textbf{Lemat: } Niech T będzie drzewem czerwono-czarnym o n węzłach. \par Wówczas wysokość drzewa T jest z góry ograniczona przez: \par
    \begin{center}
        $\text{wysokość}(T) \leq 2 \cdot log_2(n + 1)$
    \end{center}
    \vspace{1\baselineskip}
    \textbf{RB - Insert} \par
    \begin{itemize}
        \item Wstawiamy węzeł z w taki sposób jak w BST
        \item z.kolor = czerwony
        \item FixUp (nie chodzi o zespół punkowy)
    \end{itemize}
    Więcej o drzewacz czerwono - czarnych można znaleźć pod linkiem: \par 
    \begin{center}
        https://inf.ug.edu.pl/~pmp/Z/ASDwyklad/czczWUd.pdf
    \end{center} \par

\end{document} 